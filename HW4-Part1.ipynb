{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from random import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This methods accepts:\n",
    "    data - a list of words\n",
    "    labels - a list of tags (as #s) corresponding to data (defaults to none for test data)\n",
    "    tag_delim - the tag corresponding to -DOCSTART- that we split the tags on\n",
    "    word_delim - the word to split the sentences on\n",
    "Returns:\n",
    "    sentences: a list of lists. Each sublist holds the words of a given sentence\n",
    "    sentence_labels: a list of lists of the corresponding tags (as #s) to the sentences\n",
    "\n",
    "NOTE: This is called within read_data\n",
    "'''\n",
    "\n",
    "def get_sentences(data_file, word_delim = '-DOCSTART-'):\n",
    "    data = pd.read_csv(data_file)\n",
    "    del data['id']\n",
    "    sentences = []\n",
    "    for x, y in itertools.groupby(list(data['word']), lambda z: z == word_delim):\n",
    "        if x: sentences.append([])\n",
    "        sentences[-1].extend(y)\n",
    "    return sentences\n",
    "\n",
    "def get_labels(label_file):\n",
    "    labels = pd.read_csv(label_file)\n",
    "    del labels['id']\n",
    "    # convert labels to numbers and store the conversion from # back to tag in a dictionary tag_list\n",
    "    labels['tag'] = labels['tag'].astype('category')\n",
    "    tag_list = list(labels['tag'].cat.categories)\n",
    "    tag_delim = tag_list.index('O') # get # corresponding to tag 'O'\n",
    "    labels = np.array(labels['tag'].cat.codes)\n",
    "    sentence_labels = []\n",
    "    for x, y in itertools.groupby(labels, lambda z: z == tag_delim):\n",
    "        if x:\n",
    "            sentence_labels.append([])\n",
    "        sentence_labels[-1].extend(y)\n",
    "    return sentence_labels, tag_list\n",
    "'''\n",
    "This method accepts:\n",
    "    data_file and label_file (optional) - file names for words and corresponding tags\n",
    "Returns:\n",
    "    sentences - a list of sentences, where each sentence is represented as a list of words and begins with -DOCSTART-\n",
    "    sentences_tags - a list of lists of tags corresponding to the sentences, where tags are represented as integers\n",
    "    tag_list - a list of the unique tags. The index of each tag is what we replace all tags with.\n",
    "            Later, we will use this list to convert number tags back to actual tags:\n",
    "            tags = [tag_list[x] for x in tags]\n",
    "\n",
    "'''\n",
    "def read_data(data_file, label_file = None):\n",
    "    if label_file is None:\n",
    "        return get_sentences(data_file)\n",
    "    else:\n",
    "        sentences, (sentences_tags, tag_list) = get_sentences(data_file), get_labels(label_file)\n",
    "        return sentences, sentences_tags, tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Params(object):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RandomVec:\n",
    "    def __init__(self, dim):\n",
    "        self.dim = dim\n",
    "        self.vocab = {}\n",
    "        self.vec = []\n",
    "\n",
    "    def __getitem__(self, word):\n",
    "        ind = self.vocab.get(word, -1)\n",
    "        if ind == -1:\n",
    "            new_vec = np.array([random() for i in range(self.dim)])\n",
    "            self.vocab[word] = len(self.vocab)\n",
    "            self.vec.append(new_vec)\n",
    "            return new_vec\n",
    "        else:\n",
    "            return self.vec[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class WordVec:\n",
    "    def __init__(self, args):\n",
    "        print('processing corpus')\n",
    "        if args.restore is None:\n",
    "            sentences =  read_data(args.corpus)\n",
    "            print('training')\n",
    "            self.wvec_model = Word2Vec(sentences=sentences, size=args.dimension, window=args.window,\n",
    "                                       workers=args.workers,\n",
    "                                       sg=args.sg,\n",
    "                                       batch_words=args.batch_size, min_count=1, max_vocab_size=args.vocab_size)\n",
    "            self.wvec_model.save('wordvec_model_train_' + str(args.dimension) + '.pkl')\n",
    "        else:\n",
    "            #self.wvec_model = KeyedVectors.load_word2vec_format(args.restore, binary=True)\n",
    "            print('loading model')\n",
    "            self.wvec_model = Word2Vec.load(args.restore)\n",
    "        self.rand_model = RandomVec(args.dimension)\n",
    "\n",
    "    def __getitem__(self, word):\n",
    "        #word = word.lower()\n",
    "        try:\n",
    "            return self.wvec_model[word]\n",
    "        except KeyError:\n",
    "            #print(\"Don't found!\")\n",
    "            return self.rand_model[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing corpus\n",
      "training\n"
     ]
    }
   ],
   "source": [
    "args = Params()\n",
    "args.corpus = 'data/train_x.csv'\n",
    "args.dimension = word_dim\n",
    "args.window = 5\n",
    "args.vocab_size = 10000\n",
    "args.workers = 3\n",
    "args.sg = 1\n",
    "args.batch_size = 10000\n",
    "args.restore = None\n",
    "w2vmodel = WordVec(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'Load train and dev data'\n",
    "train_x, train_y, tag_list = read_data('data/train_x.csv', 'data/train_y.csv')\n",
    "#dev_x, dev_y, tag_list = read_data('data/dev_x.csv', 'data/dev_y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test_x = read_data('data/test_x.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def embed_dataset(dataset, max_sentence_length=5000):\n",
    "    output = np.zeros((len(dataset), max_sentence_length, word_dim))\n",
    "    count = 0\n",
    "    length = np.zeros(len(dataset))\n",
    "    for sentence in dataset:\n",
    "        size = min(len(sentence), max_sentence_length)\n",
    "        output[count,:size] = [w2vmodel[i] for i in sentence[:size]]\n",
    "        if (max_sentence_length > size):\n",
    "            output[count,size:max_sentence_length] = [[0] * word_dim] * (max_sentence_length - size)\n",
    "        length[count] = len(sentence)\n",
    "        count = count + 1\n",
    "    return output, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def embed_labels(labels, max_sentence_length=5000):\n",
    "    output = np.zeros((len(labels), max_sentence_length))\n",
    "    count = 0\n",
    "    for sentence in labels:\n",
    "        size = min(len(sentence), max_sentence_length)\n",
    "        output[count,:size] = sentence[:size]\n",
    "        if (max_sentence_length > size):\n",
    "            output[count,size:max_sentence_length] = [0] * (max_sentence_length - size)\n",
    "        count = count + 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embed_train_x, length_train_x = embed_dataset(train_x, max_sentence_length=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embed_train_y = embed_labels(train_y, max_sentence_length=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_train_input, embed_test_input = embed_train_x[:1100], embed_train_x[1100:]\n",
    "embed_train_output, embed_test_output = embed_train_y[:1100], embed_train_y[1100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_test_output = np.array(embed_test_output, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#embed_test_x, length_test_x = embed_dataset(test_x, max_sentence_length=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        # Define the input placeholders\n",
    "        \n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            self.input_data = tf.placeholder(tf.float32, [None, args.sentence_length, args.word_dim])\n",
    "            self.output_data = tf.placeholder(tf.int32, [None, args.sentence_length])\n",
    "            self.one_hot_output_data = tf.one_hot(self.output_data, args.class_size)\n",
    "            \n",
    "            if args1.cell_type == 1:\n",
    "                # Define the forward cell\n",
    "                fw_cell = tf.nn.rnn_cell.GRUCell(args.rnn_size, activation=tf.nn.tanh)\n",
    "                # Define the backward cell\n",
    "                bw_cell = tf.nn.rnn_cell.GRUCell(args.rnn_size, activation=tf.nn.tanh)\n",
    "            else:\n",
    "                # Define the forward cell\n",
    "                fw_cell = tf.nn.rnn_cell.LSTMCell(args.rnn_size, state_is_tuple=True)\n",
    "                # Define the backward cell\n",
    "                bw_cell = tf.nn.rnn_cell.LSTMCell(args.rnn_size, state_is_tuple=True)\n",
    "            \n",
    "            # Add Dropout to the forward and backward cell\n",
    "            if args1.dropout_layers == True:\n",
    "                fw_cell = tf.nn.rnn_cell.DropoutWrapper(fw_cell, output_keep_prob=0.5)\n",
    "                bw_cell = tf.nn.rnn_cell.DropoutWrapper(bw_cell, output_keep_prob=0.5)          \n",
    "            \n",
    "            # Add multilayers of the forward and backward layers\n",
    "            if args1.num_layers > 1:            \n",
    "                fw_cell = tf.nn.rnn_cell.MultiRNNCell([fw_cell] * args.num_layers, state_is_tuple=True)\n",
    "                bw_cell = tf.nn.rnn_cell.MultiRNNCell([bw_cell] * args.num_layers, state_is_tuple=True)\n",
    "\n",
    "            # Get the max sequence length for each sequence to adjust the size of the RNN and later on compute the score.\n",
    "            words_used_in_sent = tf.sign(tf.reduce_max(tf.abs(self.input_data), reduction_indices=2))\n",
    "            self.length = tf.cast(tf.reduce_sum(words_used_in_sent, reduction_indices=1), tf.int32)\n",
    "\n",
    "            # Create the bidirectional RNN. The input has to be unpack the input into a list of tensors of 2D, corresponding\n",
    "            # to each element of the batch thus, the permutation of dimensions.\n",
    "            output, _, _ = tf.nn.bidirectional_rnn(fw_cell, bw_cell,\n",
    "                                                   tf.unpack(tf.transpose(self.input_data, perm=[1, 0, 2])),\n",
    "                                                   dtype=tf.float32, sequence_length=self.length)\n",
    "\n",
    "            # Define the Weight and Bias of the FC layer for the prediction. The number of nodes is two times the\n",
    "            # size of the RNN to use the forward and backward direction.\n",
    "            weight, bias = self.weight_and_bias(2 * args.rnn_size, args.class_size)\n",
    "\n",
    "            # First, we permute the output to the original order of dimensions, we stack them back together into a single tensor.\n",
    "            # Finally we reshape it to concatenate the output of both directions of the Bi-RNN to operate on the last layer.\n",
    "            output = tf.reshape(tf.transpose(tf.pack(output), perm=[1, 0, 2]), [-1, 2 * args.rnn_size])\n",
    "\n",
    "            # FC calculating the prediction\n",
    "            pred = tf.matmul(output, weight) + bias\n",
    "            prediction = tf.nn.softmax(pred)\n",
    "\n",
    "            # Reshape the prediction to the maximum length of the sentence and the number of classes\n",
    "            self.prediction = tf.reshape(prediction, [-1, args.sentence_length, args.class_size])\n",
    "            \n",
    "            # Compute the cost excluding the padding elements\n",
    "            self.loss = self.cost()\n",
    "            \n",
    "            # We use the optimizer recommended in the paper with the corresponding parameters\n",
    "            optimizer = tf.train.AdamOptimizer(0.003)\n",
    "            \n",
    "            # We manually compute the gradients and clip them to avoid vanishing and exploding gradients.\n",
    "            tvars = tf.trainable_variables()\n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, tvars), 10)\n",
    "            \n",
    "            # We apply the computed gradients to the optimizer. \n",
    "            self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    def cost(self):\n",
    "        cross_entropy = self.one_hot_output_data * tf.log(self.prediction)\n",
    "        cross_entropy = -tf.reduce_sum(cross_entropy, reduction_indices=2)\n",
    "        mask = tf.sign(tf.reduce_max(tf.abs(self.one_hot_output_data), reduction_indices=2))\n",
    "        cross_entropy *= mask\n",
    "        cross_entropy = tf.reduce_sum(cross_entropy, reduction_indices=1)\n",
    "        cross_entropy /= tf.cast(self.length, tf.float32)\n",
    "        return tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    @staticmethod\n",
    "    def weight_and_bias(in_size, out_size):\n",
    "        weight = tf.truncated_normal([in_size, out_size], stddev=0.01)\n",
    "        bias = tf.constant(0.1, shape=[out_size])\n",
    "        return tf.Variable(weight), tf.Variable(bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "args1 = Params()\n",
    "args1.sentence_length = 500\n",
    "args1.word_dim = word_dim\n",
    "args1.class_size = len(tag_list)\n",
    "args1.rnn_size = 10\n",
    "args1.num_layers = 1\n",
    "args1.batch_size = 110\n",
    "args1.epoch = 51\n",
    "args1.cell_type = 2 # 1 = GRU, 2 = LSTM\n",
    "args1.dropout_layers = None\n",
    "args1.restore = None\n",
    "model = Model(args1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score(args, prediction, target, length):\n",
    "    prediction = np.argmax(prediction, 2)\n",
    "    acum = 0\n",
    "    for i in range(len(target)):\n",
    "        for j in range(length[i]):\n",
    "            if target[i, j] == prediction[i, j]:\n",
    "                acum += 1\n",
    "    return (acum / float(np.sum(length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables Initialized\n",
      "Epoch: 0\n",
      "Iter 0, Minibatch Loss= 15.8843\n",
      "test_a score:0.106987967885\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "Epoch: 10\n",
      "Iter 0, Minibatch Loss= 12.9405\n",
      "test_a score:0.245657540645\n",
      "Epoch: 11\n",
      "Epoch: 12\n",
      "Epoch: 13\n",
      "Epoch: 14\n",
      "Epoch: 15\n",
      "Epoch: 16\n",
      "Epoch: 17\n",
      "Epoch: 18\n",
      "Epoch: 19\n",
      "Epoch: 20\n",
      "Iter 0, Minibatch Loss= 10.3745\n",
      "test_a score:0.531514801059\n",
      "Epoch: 21\n",
      "Epoch: 22\n",
      "Epoch: 23\n",
      "Epoch: 24\n",
      "Epoch: 25\n",
      "Epoch: 26\n",
      "Epoch: 27\n",
      "Epoch: 28\n",
      "Epoch: 29\n",
      "Epoch: 30\n",
      "Iter 0, Minibatch Loss= 8.23315\n",
      "test_a score:0.632186464426\n",
      "Epoch: 31\n",
      "Epoch: 32\n",
      "Epoch: 33\n",
      "Epoch: 34\n",
      "Epoch: 35\n",
      "Epoch: 36\n",
      "Epoch: 37\n",
      "Epoch: 38\n",
      "Epoch: 39\n",
      "Epoch: 40\n",
      "Iter 0, Minibatch Loss= 6.47642\n",
      "test_a score:0.70976135934\n",
      "Epoch: 41\n",
      "Epoch: 42\n",
      "Epoch: 43\n",
      "Epoch: 44\n",
      "Epoch: 45\n"
     ]
    }
   ],
   "source": [
    "#train_inp, train_out = get_train_data()\n",
    "train_inp, train_out = embed_train_input, embed_train_output\n",
    "test_a_inp, test_a_out = embed_test_input, embed_test_output\n",
    "#test_b_inp, test_b_out = get_test_b_data()\n",
    "maximum = 0\n",
    "with tf.Session(graph=model.graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"Variables Initialized\")\n",
    "    saver = tf.train.Saver()\n",
    "    if args1.restore is not None:\n",
    "        saver.restore(sess, args1.restore)\n",
    "        print(\"model restored\")\n",
    "    for e in range(args1.epoch):\n",
    "        print(\"Epoch: \" + str(e))\n",
    "        for ptr in range(0, len(train_inp), args1.batch_size):\n",
    "            feed_dict = {model.input_data: train_inp[ptr:ptr + args1.batch_size],\n",
    "                                      model.output_data: train_out[ptr:ptr + args1.batch_size]}\n",
    "            _, loss = sess.run([model.train_op, model.loss], feed_dict=feed_dict)\n",
    "            if e % 10 == 0 and ptr == 0:\n",
    "                #save_path = saver.save(sess, \"model.ckpt\")\n",
    "                #print(\"model saved in file: %s\" % save_path)\n",
    "                print(\"Iter \" + str(ptr) + \", Minibatch Loss= \" + str(loss))\n",
    "                pred, length = sess.run([model.prediction, model.length], {model.input_data: test_a_inp,\n",
    "                                                                           model.output_data: test_a_out})\n",
    "                print('test_a score:' + str(score(args1, pred, test_a_out, length)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testset and output results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=model.graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"Variables Initialized\")\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, \"./model.ckpt\")\n",
    "    print(\"model restored\")\n",
    "    test_prediction, length = sess.run([model.prediction, model.length], {model.input_data: embed_test_x})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_prediction = np.argmax(test_prediction, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_file(output, length, filename):\n",
    "    f = open(filename,'w')\n",
    "    f.write(\"id,tag\\n\")\n",
    "    count = 0\n",
    "    for key, sentence in enumerate(output):\n",
    "        size = int(min(len(sentence), length[key]))\n",
    "        for word in range(size):\n",
    "            f.write(str(count) + \",\\\"\" + tag_list[sentence[word]] + \"\\\"\\n\")\n",
    "            count += 1\n",
    "        if len(sentence) < length[key]:\n",
    "            for i in range(int(length[key])-len(sentence)):\n",
    "                f.write(str(count) + \",\\\"\" + tag_list[int(random() * len(tag_list))] + \"\\\"\\n\")\n",
    "                count += 1\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generate_file(final_prediction, length_test_x, \"output.csv\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
